{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Paths\n",
    "DATA = Path(\"../data/results.csv\")   # your input CSV (we’ll create one if needed)\n",
    "OUT  = Path(\"../outputs\")            # folder for charts/summaries\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   learner_id  50 non-null     object\n",
      " 1   module      50 non-null     object\n",
      " 2   screen      50 non-null     object\n",
      " 3   completed   50 non-null     bool  \n",
      " 4   quiz_score  50 non-null     int64 \n",
      " 5   timestamp   50 non-null     object\n",
      "dtypes: bool(1), int64(1), object(4)\n",
      "memory usage: 2.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(  learner_id module screen  completed  quiz_score         timestamp\n",
       " 0         u1  Intro     s1      False          40  2025-08-01 09:00\n",
       " 1         u2  Intro     s2       True          75  2025-08-01 09:05\n",
       " 2         u3  Intro     s3       True          82  2025-08-01 09:15\n",
       " 3         u4  Intro     s2      False          55  2025-08-01 09:20\n",
       " 4         u5  Intro     s4       True          90  2025-08-01 09:30,\n",
       " None,\n",
       " learner_id    0\n",
       " module        0\n",
       " screen        0\n",
       " completed     0\n",
       " quiz_score    0\n",
       " timestamp     0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA)\n",
    "df.head(), df.info(), df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip().str.lower()\n",
    "df[\"completed\"]  = df[\"completed\"].astype(bool)\n",
    "df[\"quiz_score\"] = pd.to_numeric(df[\"quiz_score\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'completion_rate': 0.7,\n",
       " 'average_quiz_score': 72.84,\n",
       " 'top_dropoff_screen': 's2'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_rate = df[\"completed\"].mean()\n",
    "avg_quiz_score  = df[\"quiz_score\"].mean()\n",
    "dropoff_screen  = (df.loc[~df[\"completed\"], \"screen\"].value_counts().idxmax()\n",
    "                   if (~df[\"completed\"]).any() else None)\n",
    "\n",
    "kpis = {\n",
    "    \"completion_rate\": round(float(completion_rate), 3),\n",
    "    \"average_quiz_score\": round(float(avg_quiz_score), 2),\n",
    "    \"top_dropoff_screen\": dropoff_screen\n",
    "}\n",
    "kpis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completion by module\n",
    "module_completion = df.groupby(\"module\")[\"completed\"].mean().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(); module_completion.plot(kind=\"bar\")\n",
    "plt.title(\"Completion Rate by Module\"); plt.xlabel(\"Module\"); plt.ylabel(\"Completion Rate\")\n",
    "plt.tight_layout(); plt.savefig(OUT/\"completion_by_module.png\", dpi=150); plt.close()\n",
    "\n",
    "# quiz score distribution\n",
    "plt.figure(); df[\"quiz_score\"].dropna().plot(kind=\"hist\", bins=10)\n",
    "plt.title(\"Quiz Score Distribution\"); plt.xlabel(\"Quiz Score\"); plt.ylabel(\"Count\")\n",
    "plt.tight_layout(); plt.savefig(OUT/\"quiz_score_hist.png\", dpi=150); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'completion_rate': 0.7,\n",
       " 'average_quiz_score': 72.84,\n",
       " 'top_dropoff_screen': 's2',\n",
       " 'median_quiz_score': 77.5,\n",
       " 'std_quiz_score': 17.35}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_quiz = float(df[\"quiz_score\"].median())\n",
    "std_quiz    = float(df[\"quiz_score\"].std(ddof=1)) \n",
    "\n",
    "kpis.update({\n",
    "    \"median_quiz_score\": round(median_quiz, 1),\n",
    "    \"std_quiz_score\": round(std_quiz, 2),\n",
    "})\n",
    "kpis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Updated'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md = f\"\"\"# Course Analytics Summary\n",
    "\n",
    "**Completion rate:** {kpis['completion_rate']*100:.1f}%  \n",
    "**Average quiz score:** {kpis['average_quiz_score']:.1f}  \n",
    "**Median quiz score:** {kpis['median_quiz_score']:.1f}  \n",
    "**Std dev (quiz):** {kpis['std_quiz_score']:.2f}  \n",
    "**Top drop-off screen:** {kpis['top_dropoff_screen']}\n",
    "\"\"\"\n",
    "(OUT/\"summary.md\").write_text(md, encoding=\"utf-8\")\n",
    "\"Updated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'completion_rate': 0.7,\n",
       " 'average_quiz_score': 72.84,\n",
       " 'top_dropoff_screen': 's2',\n",
       " 'median_quiz_score': 77.5,\n",
       " 'std_quiz_score': 17.35,\n",
       " 'ttest_quiz_by_module': {'modules': ['Intro', 'ModuleA'],\n",
       "  't_stat': -0.34830995826198524,\n",
       "  'p_value': 0.7306442212906292}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "mods = df[\"module\"].dropna().unique()\n",
    "ttest_result = None\n",
    "\n",
    "if len(mods) >= 2:\n",
    "    a, b = mods[:2]\n",
    "    a_scores = df.loc[df[\"module\"]==a, \"quiz_score\"].dropna()\n",
    "    b_scores = df.loc[df[\"module\"]==b, \"quiz_score\"].dropna()\n",
    "    if len(a_scores) > 1 and len(b_scores) > 1:\n",
    "        t, p = stats.ttest_ind(a_scores, b_scores, equal_var=False)\n",
    "        ttest_result = {\"modules\":[a,b], \"t_stat\": float(t), \"p_value\": float(p)}\n",
    "\n",
    "kpis[\"ttest_quiz_by_module\"] = ttest_result\n",
    "kpis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'completion_rate': 0.7,\n",
       " 'average_quiz_score': 72.84,\n",
       " 'top_dropoff_screen': 's2',\n",
       " 'median_quiz_score': 77.5,\n",
       " 'std_quiz_score': 17.35,\n",
       " 'ttest_quiz_by_module': {'modules': ['Intro', 'ModuleA'],\n",
       "  't_stat': -0.34830995826198524,\n",
       "  'p_value': 0.7306442212906292},\n",
       " 'ml_completion_accuracy': 0.667,\n",
       " 'ml_completion_auc': 0.3}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# simple features (avoid leakage: don’t include quiz_score)\n",
    "df[\"screen_idx\"] = df[\"screen\"].astype(str).str.extract(r\"(\\d+)\", expand=False).astype(float)\n",
    "df[\"timestamp\"]  = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "df[\"hour\"]       = df[\"timestamp\"].dt.hour\n",
    "df[\"weekday\"]    = df[\"timestamp\"].dt.weekday\n",
    "\n",
    "X_cats = pd.get_dummies(df[\"module\"], prefix=\"module\", dummy_na=True)\n",
    "features = pd.concat([df[[\"screen_idx\",\"hour\",\"weekday\"]], X_cats], axis=1).fillna(0)\n",
    "target   = df[\"completed\"].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, stratify=target, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_prob) if len(y_test.unique())==2 else None\n",
    "\n",
    "kpis.update({\n",
    "    \"ml_completion_accuracy\": round(float(acc),3),\n",
    "    \"ml_completion_auc\": round(float(auc),3) if auc is not None else None\n",
    "})\n",
    "kpis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure()\n",
    "plt.imshow(cm, interpolation=\"nearest\")\n",
    "plt.title(\"Completion Classifier — Confusion Matrix\")\n",
    "plt.xticks([0,1], [\"Pred 0\",\"Pred 1\"]); plt.yticks([0,1], [\"True 0\",\"True 1\"])\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "plt.tight_layout(); plt.savefig(OUT/\"cm_completion.png\", dpi=150); plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Updated'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md = f\"\"\"# Course Analytics Summary\n",
    "\n",
    "## Statistics\n",
    "- Completion rate: {kpis['completion_rate']*100:.1f}%\n",
    "- Average quiz score: {kpis['average_quiz_score']:.1f}\n",
    "- Median quiz score: {kpis['median_quiz_score']:.1f}\n",
    "- Std dev (quiz): {kpis['std_quiz_score']:.2f}\n",
    "- Top drop-off screen: {kpis['top_dropoff_screen']}\n",
    "- t-test (first two modules): {kpis['ttest_quiz_by_module']}\n",
    "\n",
    "## ML (demo)\n",
    "- Model: Logistic Regression for completion (features: screen_idx, hour, weekday, module dummies)\n",
    "- Accuracy: {kpis.get('ml_completion_accuracy')}\n",
    "- AUC: {kpis.get('ml_completion_auc')}\n",
    "\n",
    "## Notes & Caveats\n",
    "- Dataset is small and synthetic — results are illustrative, not production-ready.\n",
    "- Next steps: add richer features (e.g. time-on-task, attempts), try cross-validation, and tune hyperparameters.\n",
    "\"\"\"\n",
    "(OUT/\"summary.md\").write_text(md, encoding=\"utf-8\")\n",
    "\"Updated\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
